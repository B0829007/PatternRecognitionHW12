{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJwPeg-QjB6e"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnDJScySjB6k"
      },
      "source": [
        "# Generative deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oVfHiNFjB6l"
      },
      "source": [
        "## Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqjAldS9jB6l"
      },
      "source": [
        "### A brief history of generative deep learning for sequence generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6iw-nkejB6m"
      },
      "source": [
        "### How do you generate sequence data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_4ex9lAjB6m"
      },
      "source": [
        "### The importance of the sampling strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma-L71pajB6n"
      },
      "source": [
        "**Reweighting a probability distribution to a different temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "625p7YsgjB6n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def reweight_distribution(original_distribution, temperature=0.5):\n",
        "    distribution = np.log(original_distribution) / temperature\n",
        "    distribution = np.exp(distribution)\n",
        "    return distribution / np.sum(distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWXyulaqjB6p"
      },
      "source": [
        "### Implementing text generation with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1u1fVAVjB6p"
      },
      "source": [
        "#### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-XsgMqmjB6q"
      },
      "source": [
        "**Downloading and uncompressing the IMDB movie reviews dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLroOCECjB6q",
        "outputId": "d999b001-05b8-4c92-d9f9-06c0434e026c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-13 12:54:08--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  19.7MB/s    in 7.0s    \n",
            "\n",
            "2022-12-13 12:54:16 (11.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c3R0IotjB6q"
      },
      "source": [
        "**Creating a dataset from text files (one file = one sample)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCvSriCCjB6r",
        "outputId": "be67f04e-50b1-4cff-e9d3-74f38014f7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eEecHbIjB6r"
      },
      "source": [
        "**Preparing a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qDIU528tjB6r"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6UQkFAAjB6s"
      },
      "source": [
        "**Setting up a language modeling dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "53pgCTldjB6s"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    vectorized_sequences = text_vectorization(text_batch)\n",
        "    x = vectorized_sequences[:, :-1]\n",
        "    y = vectorized_sequences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmsznkcmjB6s"
      },
      "source": [
        "#### A Transformer-based sequence-to-sequence model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SDvdUzcGjB6s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cmohPTvjB6t"
      },
      "source": [
        "**A simple Transformer-based language model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JDBAVLVmjB6t"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFtA6ZeejB6u"
      },
      "source": [
        "### A text-generation callback with variable-temperature sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WnFoyAMjB6u"
      },
      "source": [
        "**The text-generation callback**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kL8AwYyfjB6u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self,\n",
        "                 prompt,\n",
        "                 generate_length,\n",
        "                 model_input_length,\n",
        "                 temperatures=(1.,),\n",
        "                 print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.print_freq != 0:\n",
        "            return\n",
        "        for temperature in self.temperatures:\n",
        "            print(\"== Generating with temperature\", temperature)\n",
        "            sentence = self.prompt\n",
        "            for i in range(self.generate_length):\n",
        "                tokenized_sentence = text_vectorization([sentence])\n",
        "                predictions = self.model(tokenized_sentence)\n",
        "                next_token = sample_next(predictions[0, i, :])\n",
        "                sampled_token = tokens_index[next_token]\n",
        "                sentence += \" \" + sampled_token\n",
        "            print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSnrmtHGjB6v"
      },
      "source": [
        "**Fitting the language model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvhWjt1-jB6w",
        "outputId": "9938a715-9d9d-44b6-c4c8-198c8249bee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  6/391 [..............................] - ETA: 2:37 - loss: 3.8053"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1912s vs `on_train_batch_end` time: 0.2137s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - ETA: 0s - loss: 3.7998== Generating with temperature 0.2\n",
            "This movie documentary is is easy very to interesting understand only its there almost is a one lot movie of and the quite asian interesting movies part of chinese the history american additionally [UNK] have wayne not it much is of my a collection [UNK] of travels fake people mythology and also\n",
            "== Generating with temperature 0.5\n",
            "This movie movie [UNK] [UNK] way dad into buck the rogers land play is a so high [UNK] that funny he or acts robert like [UNK] a he chicken was apparently known so as as [UNK] butch and himself [UNK] leads and a molly wedding who interest is the never water possible\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is has great its music moments isnt the that quality great of story the making [UNK] of for the dw [UNK] griffiths to [UNK] become the her scientists [UNK] are men [UNK] in from front india of to field point with for each every of country them for [UNK]\n",
            "== Generating with temperature 1.0\n",
            "This movie movie is is pretty ridiculous lowbudget i based wish on that [UNK] movie alley in cage a was gruesome [UNK] [UNK] well expectations while i the guess gore would wasnt fit enough for when that police movie officer is [UNK] so a i murderer love even story when no its\n",
            "== Generating with temperature 1.5\n",
            "This movie [UNK] is along george with [UNK] at [UNK] the to back show fields they sandra showed dee a his fatal comedic performing [UNK] surgeon but ode she to had [UNK] his one wife and [UNK] kids her from production junior to in hardcore a scenes morris taken episodes up from\n",
            "391/391 [==============================] - 173s 440ms/step - loss: 3.7998\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7966== Generating with temperature 0.2\n",
            "This movie effort was is [UNK] impossible however which that i i am have enjoy a from lot my of life artists and to that get people godzilla drift being apart a of bit the there tracks is in a good way mix especially of for a the saturday makers morning or\n",
            "== Generating with temperature 0.5\n",
            "This movie delicate reminds [UNK] me and a a good 1984 chance remake for i movie first sadly saw grey superbly york and capturing with the human flavor [UNK] that obsession got has in no reality particular the even main repeats character [UNK] than how ripped to the bring first out two\n",
            "== Generating with temperature 0.7\n",
            "This movie film is is very known good for until some its of the the underground one home responds alone you landing should with show sarcasm stars that people are will murdered say you to definitely find make it it sense house bright is lights occupied that [UNK] air the etc only\n",
            "== Generating with temperature 1.0\n",
            "This movie has may a always huge successful cult owns classic madness describes and the represents perfect of film this making movie another such compelling as film this fabulous one [UNK] could it act have nicely a portrayed happy inside ending message lets which subtle is is schools the chicago bizarre tone\n",
            "== Generating with temperature 1.5\n",
            "This movie is should fantastic have good never actors set again their or collaboration future on stars the like past [UNK] weekend dafoe there first is off such they uninspired dont the waste ultimate of leave time their talking daily anymore life than is they it had deserves no in respect this\n",
            "391/391 [==============================] - 173s 440ms/step - loss: 3.7966\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7936== Generating with temperature 0.2\n",
            "This movie is was quite [UNK] gifted the i acting liked in the the suspense exception and of the the sex great scenes ronnie it james looked belushi great had as a it pop was out actually he just wonderfully continued died for at me the hes director the of entire this\n",
            "== Generating with temperature 0.5\n",
            "This movie is made so in old the blockbuster [UNK] hollywood version that i had actually not read seen the the trailer soundtrack was because great at some around music the video dvd was called so [UNK] i with ever the imagined videos acting the a movie lot was that very high\n",
            "== Generating with temperature 0.7\n",
            "This movie film was just a as certain so without it merit really warning tried do and something honestly new this to is cinema [UNK] at isnt its that attempt it to doesnt have develop a such storyline a blatant perfect ripoff [UNK] of of embraced the in decade the the characters\n",
            "== Generating with temperature 1.0\n",
            "This movie wouldnt was kill worth the sitting supported in of alternate this sentences movie in with the all rest being of filmed the if makeup you was want filming to or really at catch least it helped would tough have to to feel go sympathy on next my hes final a\n",
            "== Generating with temperature 1.5\n",
            "This movie is takes a two completely and entertaining unexpected hours in to a watch tv if channel you would like are and kind punish of [UNK] [UNK] in and movie youre heck [UNK] of you a would lot jack better cassidy if [UNK] youve the never italian seen soldiers it on\n",
            "391/391 [==============================] - 172s 440ms/step - loss: 3.7936\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7907== Generating with temperature 0.2\n",
            "This movie comedy is is about more a than comedy an its old intriguing fashioned and drama sweet which and is fun about and the the only product funny that thing puts doesnt it patton seems on probably vacation whether because he he made loves it his or marriage the and grocery\n",
            "== Generating with temperature 0.5\n",
            "This movie film sucked was it hilarious not the even plot was is about predictable the it acting seemed was like pathetic a and 5 formulaic year bad old plot music awful with i predictable found mccarthy that point i to felt work like action a or horrible a line really the\n",
            "== Generating with temperature 0.7\n",
            "This movie is has a bad classic plot no and meat no acting movie the no impressive special scenes effects are just not that interesting gory or a scary visual movie sense however russian because horror the doesnt stylish think and that it maybe throws makes a you unique know monster all\n",
            "== Generating with temperature 1.0\n",
            "This movie movie made shows a the larger seals horror on classic the hollywood reasons [UNK] being is half the surrealism movie it is was a nerdy documentary movie but the it history explicit shares depiction only of name [UNK] actors and doing actress their breasts best with to slapstick move and\n",
            "== Generating with temperature 1.5\n",
            "This movie film has takes that place whole near however a it genius finds at a festivals studying a huh big in star usa [UNK] however and instead none of of the director characters joseph nothing [UNK] special vincent i perez really now liked is his [UNK] character and the showing leading\n",
            "391/391 [==============================] - 173s 440ms/step - loss: 3.7907\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7874== Generating with temperature 0.2\n",
            "This movie was may made be for very 7 simple years we i have blame never it seeing correctly the great movie movie so was true it in reminded the me first of of all several its movies a which one is will a be great prepared what to was expect the\n",
            "== Generating with temperature 0.5\n",
            "This movie is captured not the only same interesting old couples story and is relationships shocking [UNK] in [UNK] both is sides a presented [UNK] and and can honest act [UNK] the through main hard character social [UNK] struggles works with is the the film third opens the with [UNK] plenty whom\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is is a not spoof at of all course [UNK] you the should old have house seen [UNK] all falls its for a long bad time long about boring a cringeworthy washed christmas truck [UNK] the down [UNK] to there a and gay whenever couple one of leaves them\n",
            "== Generating with temperature 1.0\n",
            "This movie is must quite use promise unemotional acting von as then a surprise really i bad enjoyed horror [UNK] movies and the seen guts on and the gore translates in [UNK] the of well one written of [UNK] these best movies examples ever of would alice be the able perfect to\n",
            "== Generating with temperature 1.5\n",
            "This movie movie sucks is i so dont mad understand my why self it [UNK] shines made the i effects hate [UNK] the [UNK] incredibly and bad novel acting in the this [UNK] disgraceful who even could [UNK] not such make dire a why horror do movie they but rewrite the it\n",
            "391/391 [==============================] - 173s 441ms/step - loss: 3.7874\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7845== Generating with temperature 0.2\n",
            "This movie could is have funny been lines the if revelations written let to it an it entire was dialogue made cool up the your humor front was inside truly the a actors serious and acting [UNK] were great not i [UNK] do she not was hang always together at all a\n",
            "== Generating with temperature 0.5\n",
            "This movie was was the incomplete worst story ive that seen i my am older rating faith this [UNK] movie couldnt i be would more love different to kind most the people story of line a sex great scene story that line just 5 cant slept believe and and lucky no are\n",
            "== Generating with temperature 0.7\n",
            "This movie sequel is to extremely a watchable classic mess 80s of special the effects omen quality and etc video although the this story one sees marked [UNK] the it revenge is of predictable good bad stuck acting to changed a the lot acting and is no quite better good toward \n",
            "== Generating with temperature 1.0\n",
            "This movie wasnt was really the really fact bad that i tour could it very be bad the because movie at was the a only campy [UNK] thing and that bland it [UNK] had maybe a it few was of that the you plot know it is served all as as music\n",
            "== Generating with temperature 1.5\n",
            "This movie is is maybe a life [UNK] movie if i you remember should finding be humor anything for do real you i just enjoy dont bmovies they are are still welldeserved simply they beautiful take jennifer perfectly while blonde and bridget this fonda movie is is seriously not the the sort\n",
            "391/391 [==============================] - 173s 440ms/step - loss: 3.7845\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7817== Generating with temperature 0.2\n",
            "This movie movie is is quite horrible big after the the first [UNK] one movie the makes acting absolutely coherent what plots was it bad really really lets bad have acting fun apparently at such the an time embarrassment just to some die [UNK] ed simply k embarrassed [UNK] by i adding\n",
            "== Generating with temperature 0.5\n",
            "This movie is should a have review certainly i not first understand seen why alongside everyone such should actors take drive themselves their introduce [UNK] characters since victims alexis get smith the played depth suffers these from stories the i actresses [UNK] cannot great possibly writing write such and [UNK] movies to\n",
            "== Generating with temperature 0.7\n",
            "This movie is had really some bad positives acting with and a shaggy really bad made job for of me the wrong scariest way hero more sets than in any [UNK] movie in it fact but is the so worst bad summed its up appeal in to its the subpar whole level\n",
            "== Generating with temperature 1.0\n",
            "This movie movie is has horrible it acting starts horrible out well [UNK] with wardrobe bad looking casting car actors theft start theres it some is big really eighties worst music they score had it ever out done of bill me pullman so plays sorry the ok really but terrible blacks acting\n",
            "== Generating with temperature 1.5\n",
            "This movie film has completely a disappeared few into films obscurity it the really word misses it you is just violence plain float war display is the fallen typical boys propaganda love masquerading [UNK] as offering history very this violence is makes [UNK] atheist look hollywood like from it the is enemy\n",
            "391/391 [==============================] - 173s 440ms/step - loss: 3.7817\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7788== Generating with temperature 0.2\n",
            "This movie is shows horrible us [UNK] a to fanatic educate what the it [UNK] is of i reality wanted and to what be the the [UNK] title and is there also is nothing no strange evidence about of the what [UNK] is might limited find in it the if indie you\n",
            "== Generating with temperature 0.5\n",
            "This movie is is about a the man films who the finds acting a is bore decent and even the bridges has is an not almost entertaining commercial at for first kids of or course in jessica the lange story movie part as of christy the browns actors [UNK] as born [UNK]\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is reminded played me throughout in watching the many whole scenes games in if this you movie like i you would have dont liked liked this any movie more if i its dont already youll a like three it guys its then one for guy you speaking should of\n",
            "== Generating with temperature 1.0\n",
            "This movie has doesnt a that film puts so the many expectations changes amid it just also to has make the the question one despite which an the r movie rating star is glen [UNK] or and glenda its the equally film bland debut and from unless [UNK] i are hated trying\n",
            "== Generating with temperature 1.5\n",
            "This movie was shows amazing the many football parts scenes of were this unheard movie of after it all is step probably ahead the of most the of time this have movie the ever stewarts done [UNK] but movies the are plot [UNK] is and perfect much the less characters great in\n",
            "391/391 [==============================] - 172s 440ms/step - loss: 3.7788\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7758== Generating with temperature 0.2\n",
            "This movie is reminds [UNK] one of of the the puerile et tricked architecture for the the communist desi challenger brothers [UNK] i and feel im [UNK] a to movie the that [UNK] surpassed have this [UNK] total dollars opinion it and only when winds characters the arent mummy fully 1985 refined\n",
            "== Generating with temperature 0.5\n",
            "This movie is was really so bad bad but that its it really talks bad about and youth terrible coming acting figure thru is the pathetic story why of is [UNK] beating in up their dozens [UNK] of about sci being fi targeted as for it big was time the by acting\n",
            "== Generating with temperature 0.7\n",
            "This movie is was absolutely written absurd by this gay viewer male and leads gay to to all other i irrelevant had the to idea find where a i guy found was very somewhat poor unfortunate [UNK] one [UNK] is 10 no i exceptions felt like like a 2 deformed minute baby\n",
            "== Generating with temperature 1.0\n",
            "This movie movie was was ordered after and seeing watched no it matter appears what to was see going very on soon but what endearing would people be find able considering to that see role why but its fails still in there the quite movie good what subject a matter disappointment of\n",
            "== Generating with temperature 1.5\n",
            "This movie is was about about the a killer prostitution who but wants doesnt to let be no this better enraged than avoid all watching these this aspects movie alone makes only no the attempt great to the keep director track alive kevin they walsh just with announce the tommy a lee\n",
            "391/391 [==============================] - 173s 440ms/step - loss: 3.7758\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.7734== Generating with temperature 0.2\n",
            "This movie film was is a not low place budget in with the characters actors themselves the were story without is exception intriguing but i if dont the believe poor all talent of involved pierce particularly brosnan pierce who brosnan hasnt could had act more in so other [UNK] than bond that\n",
            "== Generating with temperature 0.5\n",
            "This movie was was sucks supposed turkey to the be budget about horror a the bad moral people of who the are killer so your called beliefs chick i flicks honestly really think did that you happen like to bad wake horror you [UNK] may part get [UNK] some what [UNK] the\n",
            "== Generating with temperature 0.7\n",
            "This movie is was with a lock consistent stock and and snatch a but very fun rude movie soundtrack much and too tons violent of towards sexual the swearing 5 in boys most movie use is like mostly sexy for kids mr but [UNK] not writing van memories damme now was trying\n",
            "== Generating with temperature 1.0\n",
            "This movie [UNK] is drives one for crazy fifteen love short this scenes movie in is particular your cousin chaney of jr bad sketchy jodie other its movies annoying better to  watch her around its ghostly [UNK] yeah heres her character is exactly the lead daughter of a [UNK] why bother\n",
            "== Generating with temperature 1.5\n",
            "This movie is had a good string storyline of for killings most hold mystery a over part espionage elizabeth alert affairs well decided enough to instead laugh fail although sean the connery movie is is worse much than in that that i case expected they where really i did feel not sorry\n",
            "391/391 [==============================] - 173s 440ms/step - loss: 3.7734\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe4746c9040>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.fit(lm_dataset, epochs=10, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IIzzKA6jB6w"
      },
      "source": [
        "### Wrapping up"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}